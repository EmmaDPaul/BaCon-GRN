{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Inputs should be the gene network obtained from GRN inference techniques. It should have the columns \"Gene1\",\"Gene2\", \"EdgeWeight\" and \"Direction\". The sample files are provided in the data folder."
      ],
      "metadata": {
        "id": "plmLFFcVfkeI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIK5GrmVderN"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "def contains_evaluation_results_bayesian(subfolder):\n",
        "    for root, dirs, files in os.walk(subfolder):  # Walk through subfolders and sub-subfolders\n",
        "        if \"evaluation_results_BCU.csv\" in files:\n",
        "            return True  # File found, return True to exclude the subfolder\n",
        "def smart_read_csv(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        # Read a small portion to detect delimiter\n",
        "        sample = f.read(2048)\n",
        "        sniffer = csv.Sniffer()\n",
        "        try:\n",
        "            dialect = sniffer.sniff(sample, delimiters=[',', '\\t'])\n",
        "        except csv.Error:\n",
        "            # Fallback if delimiter can't be detected\n",
        "            dialect = csv.get_dialect('excel')  # defaults to comma\n",
        "        f.seek(0)\n",
        "        return pd.read_csv(f, sep=dialect.delimiter)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import itertools\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve, auc, precision_score, recall_score, f1_score, matthews_corrcoef, log_loss, cohen_kappa_score, top_k_accuracy_score\n",
        "import numpy as np\n",
        "from scipy.stats import beta\n",
        "\n",
        "def compute_method_weights(method_auprs):\n",
        "    \"\"\"Normalize AUPR values to create weights for Bayesian updating.\"\"\"\n",
        "    min_aupr, max_aupr = min(method_auprs.values()), max(method_auprs.values())\n",
        "    method_weights = {method: (aupr - min_aupr) / (max_aupr - min_aupr) + 0.1 for method, aupr in method_auprs.items()}  # Avoid zero weights\n",
        "    return method_weights\n",
        "\n",
        "method_alphas = {'GRNBOOST2': 0.55, 'SINCERITIES': 0.42, 'GENIE3': 0.56, 'LEAP': 0.43}\n",
        "def merge_edges_with_bayesian_inference(methods, method_names, credibility_threshold=0.9):\n",
        "    \"\"\"Merge edges from multiple methods and apply Bayesian inference based on Direction and EdgeWeight.\"\"\"\n",
        "\n",
        "    # Compute Bayesian weights for methods\n",
        "    method_weights = compute_method_weights(method_auprs)\n",
        "\n",
        "    # Merge DataFrames on \"Direction\" to align same edges\n",
        "    merged_df = methods[0].copy()  # Start with first method's DataFrame\n",
        "    merged_df[\"Method\"] = method_names[0]  # Assign method name\n",
        "\n",
        "    for i in range(1, len(methods)):\n",
        "        df = methods[i].copy()\n",
        "        df[\"Method\"] = method_names[i]  # Assign method name\n",
        "        merged_df = pd.concat([merged_df, df], ignore_index=True)  # Append DataFrames\n",
        "\n",
        "    # Group by 'Direction' to aggregate EdgeWeight across methods\n",
        "    grouped = merged_df.groupby(\"Direction\")\n",
        "\n",
        "    # Bayesian update function\n",
        "    def bayesian_update(edge_weights, method_names, method_weights):\n",
        "        alpha_prior = 1\n",
        "        beta_prior = 1\n",
        "        threshold = np.median(edge_weights)\n",
        "        # Compute weighted positive and negative support\n",
        "        positive_support = sum(method_weights[method] for edge, method in zip(edge_weights, method_names) if edge > threshold)\n",
        "        negative_support = sum(method_weights[method] for edge, method in zip(edge_weights, method_names) if edge <= threshold)\n",
        "\n",
        "        return alpha_prior + positive_support, beta_prior + negative_support\n",
        "\n",
        "    edge_summary = []\n",
        "    for name, group in grouped:\n",
        "        edge_weights = group[\"EdgeWeight\"].values  # Extract edge weights\n",
        "        method_names_list = group[\"Method\"].values  # Extract corresponding method names\n",
        "        a, b = bayesian_update(edge_weights, method_names_list, method_weights)\n",
        "        prob = beta.mean(a, b)  # Compute mean probability from Beta distribution\n",
        "        edge_summary.append([name, a, b, prob])\n",
        "\n",
        "    # Create a DataFrame with Bayesian results\n",
        "    edge_df = pd.DataFrame(edge_summary, columns=[\"Direction\", \"Alpha\", \"Beta\", \"Probability\"])\n",
        "\n",
        "    # Merge back \"Gene1\" and \"Gene2\" using the original merged_df\n",
        "    edge_df = edge_df.merge(merged_df[[\"Direction\", \"Gene1\", \"Gene2\"]].drop_duplicates(), on=\"Direction\", how=\"left\")\n",
        "\n",
        "    # Filter edges based on probability threshold\n",
        "    credible_edges = edge_df[edge_df[\"Probability\"] >= credibility_threshold]\n",
        "\n",
        "    return credible_edges\n",
        "\n",
        "def load_files(basefolder):\n",
        "    \"\"\"Load all CSV files from the basefolder and normalize edge weights.\"\"\"\n",
        "    all_files = glob.glob(os.path.join(basefolder, \"*.csv\"))\n",
        "    method_data = {}\n",
        "\n",
        "    for file in all_files:\n",
        "        method_name = os.path.basename(file).split('.')[0]\n",
        "        df = smart_read_csv(file)\n",
        "        print(df.columns)\n",
        "        if \"Direction\" not in df.columns:\n",
        "            # Determine which case of Gene1 and Gene2 is present\n",
        "            gene1_col = \"Gene1\" if \"Gene1\" in df.columns else (\"gene1\" if \"gene1\" in df.columns else None)\n",
        "            gene2_col = \"Gene2\" if \"Gene2\" in df.columns else (\"gene2\" if \"gene2\" in df.columns else None)\n",
        "\n",
        "            # Ensure both columns are found\n",
        "            if gene1_col and gene2_col:\n",
        "                # Create the 'Direction' column\n",
        "                df[\"Direction\"] = df[gene1_col].astype(str) + \" -> \" + df[gene2_col].astype(str)\n",
        "            else:\n",
        "                raise ValueError(\"Required columns 'Gene1'/'gene1' and/or 'Gene2'/'gene2' are missing.\")\n",
        "        print(df)\n",
        "\n",
        "        if \"EdgeWeight\" in df.columns:\n",
        "            min_val = df[\"EdgeWeight\"].min()\n",
        "            max_val = df[\"EdgeWeight\"].max()\n",
        "            # Avoid division by zero\n",
        "            if max_val > min_val:\n",
        "                df[\"EdgeWeight\"] = (df[\"EdgeWeight\"] - min_val) / (max_val - min_val)\n",
        "            else:\n",
        "                df[\"EdgeWeight\"] = 0.5  # Arbitrary constant if all weights are equal\n",
        "\n",
        "        method_data[method_name] = df\n",
        "        # print(df)\n",
        "\n",
        "    return method_data\n",
        "\n",
        "\n",
        "def evaluate_edges(predicted_edges, ref_network):\n",
        "    \"\"\"Compute AUPR and AUROC given predicted edges and the reference network.\"\"\"\n",
        "    true_edges = set(ref_network[\"Direction\"])  # Set of true edges\n",
        "    predicted_edges_list = list(zip(predicted_edges[\"Direction\"], predicted_edges[\"Probability\"]))\n",
        "\n",
        "    # Create binary labels (1 if edge exists in reference, else 0)\n",
        "    y_true = [1 if direction in true_edges else 0 for direction, _ in predicted_edges_list]\n",
        "    y_scores = [prob for _, prob in predicted_edges_list]\n",
        "\n",
        "    # Compute AUPR\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
        "    aupr = auc(recall, precision)\n",
        "\n",
        "    # Compute AUROC\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "    auroc = auc(fpr, tpr)\n",
        "\n",
        "    threshold = 0.5  # Default threshold for binary classification\n",
        "    y_pred = [1 if prob >= threshold else 0 for prob in y_scores]\n",
        "\n",
        "    precision_metric = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall_metric = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1_metric = f1_score(y_true, y_pred, zero_division=0)\n",
        "    mcc_metric = matthews_corrcoef(y_true, y_pred)\n",
        "    log_loss_metric = log_loss(y_true, y_scores, labels=[0, 1]) if len(set(y_true)) > 1 else None\n",
        "    kappa_metric = cohen_kappa_score(y_true, y_pred)\n",
        "    top_k_acc = top_k_accuracy_score(y_true, np.array(y_scores).reshape(-1, 1), k=1)  # Top-1 Accuracy\n",
        "\n",
        "    return {\n",
        "        \"AUPR\": aupr,\n",
        "        \"AUROC\": auroc,\n",
        "        \"Precision\": precision_metric,\n",
        "        \"Recall\": recall_metric,\n",
        "        \"F1-Score\": f1_metric,\n",
        "        \"MCC\": mcc_metric,\n",
        "        \"Log Loss\": log_loss_metric,\n",
        "        \"Kappa\": kappa_metric,\n",
        "        \"Top-K Acc\": top_k_acc\n",
        "    }\n",
        "\n",
        "    # return aupr, auroc\n",
        "\n",
        "def main(basefolder, ref_network_file, credibility_threshold=0.9):\n",
        "    \"\"\"Run Bayesian inference and evaluate merged networks.\"\"\"\n",
        "    method_data = load_files(basefolder)\n",
        "    ref_network = smart_read_csv(os.path.join(basefolder, ref_network_file))\n",
        "    if \"Direction\" not in ref_network.columns:\n",
        "            # Determine which case of Gene1 and Gene2 is present\n",
        "            gene1_col = \"Gene1\" if \"Gene1\" in ref_network.columns else (\"gene1\" if \"gene1\" in ref_network.columns else None)\n",
        "            gene2_col = \"Gene2\" if \"Gene2\" in ref_network.columns else (\"gene2\" if \"gene2\" in ref_network.columns else None)\n",
        "\n",
        "            # Ensure both columns are found\n",
        "            if gene1_col and gene2_col:\n",
        "                # Create the 'Direction' column\n",
        "                ref_network[\"Direction\"] = ref_network[gene1_col].astype(str) + \" -> \" + ref_network[gene2_col].astype(str)\n",
        "            else:\n",
        "                raise ValueError(\"Required columns 'Gene1'/'gene1' and/or 'Gene2'/'gene2' are missing.\")\n",
        "\n",
        "    excluded_files = {\"Final_XGBoost_MPCM\", \"refNetwork\", \"coffee_rankedEdges\", \"Final_XGBoost_PCM\"}\n",
        "    method_names = [name for name in method_data.keys() if name not in excluded_files]\n",
        "\n",
        "    results = []\n",
        "    I = 0\n",
        "    MAX_COMBO_SIZE = 5  # Limit to top-5 method permutations\n",
        "\n",
        "    for r in range(2, min(len(method_names), MAX_COMBO_SIZE) + 1):\n",
        "        for combo in itertools.combinations(method_names, r):  # Use permutations instead of combinations\n",
        "            # print(f\"Processing permutation: {perm}\")\n",
        "            I += 1\n",
        "            selected_methods = [method_data[method] for method in combo]\n",
        "\n",
        "            # Apply Bayesian inference\n",
        "            merged_edges = merge_edges_with_bayesian_inference(selected_methods, combo, credibility_threshold=credibility_threshold)\n",
        "\n",
        "            # Evaluate the merged edges\n",
        "            metrics = evaluate_edges(merged_edges, ref_network)\n",
        "            results.append({\"Combination\": combo, **metrics})\n",
        "\n",
        "\n",
        "    # Save results\n",
        "    results_df = pd.DataFrame(results)\n",
        "    output_dir = os.path.join(basefolder, \"myresult\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_file = os.path.join(output_dir, \"evaluation_results_BCU.csv\")\n",
        "    results_df.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"Evaluation results saved to {output_file}\")\n",
        "    print(\"Total I =\", I)\n",
        "\n",
        "def process_all_subfolders(main_function, base_folder, csv_file, weight_threshold):\n",
        "    subfolders = [\n",
        "        os.path.join(base_folder, name)\n",
        "        for name in os.listdir(base_folder)\n",
        "        if os.path.isdir(os.path.join(base_folder, name))\n",
        "        and not (name.endswith(\"-50\") or name.endswith(\"-70\"))\n",
        "        and not contains_evaluation_results_bayesian(os.path.join(base_folder, name))\n",
        "    ]\n",
        "\n",
        "    for subfolder in subfolders:\n",
        "        print(f\"Processing folder: {subfolder}\")\n",
        "        main_function(subfolder, csv_file, weight_threshold)\n",
        "\n",
        "# Run Bayesian inference for each dataset\n",
        "names = [\"GSD\"]\n",
        "for name in names:\n",
        "    print(name)\n",
        "    process_all_subfolders(main_function=main, base_folder=\"your folder path\" + name, csv_file=\"reference network path\", weight_threshold=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "hm67khMXdqaQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}