{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORceRJqcfs-o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import itertools\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve, auc, precision_score, recall_score, f1_score, matthews_corrcoef, log_loss, cohen_kappa_score, top_k_accuracy_score\n",
        "\n",
        "# AUPR values from previous script\n",
        "method_alphas = {'GRNBOOST2': 0.55, 'SINCERITIES': 0.42, 'GENIE3': 0.56, 'LEAP': 0.43}\n",
        "\n",
        "def contains_evaluation_results_mrf(subfolder):\n",
        "    \"\"\"Check if the evaluation results file already exists in a subfolder.\"\"\"\n",
        "    for root, dirs, files in os.walk(subfolder):\n",
        "        if \"evaluation_results_MRF_Fuse.csv\" in files:\n",
        "            return True\n",
        "\n",
        "def compute_method_weights(method_auprs):\n",
        "    \"\"\"Compute normalized weights for each method based on AUPR values.\"\"\"\n",
        "    min_aupr, max_aupr = min(method_auprs.values()), max(method_auprs.values())\n",
        "    method_weights = {method: (aupr - min_aupr) / (max_aupr - min_aupr) + 0.1 for method, aupr in method_auprs.items()}\n",
        "    return method_weights\n",
        "\n",
        "def mrf_optimization(edge_weights, method_names, method_weights):\n",
        "    \"\"\"Perform Markov Random Field optimization on edge weights.\"\"\"\n",
        "    if len(edge_weights) < 2:\n",
        "        return edge_weights  # Not enough data to optimize\n",
        "\n",
        "    method_specific_weights = np.array([method_weights[method] for method in method_names])\n",
        "\n",
        "    def energy_function(weights):\n",
        "        smoothness = np.sum((weights[:-1] - weights[1:]) ** 2) if len(weights) > 1 else 0  # Encourage smooth transitions\n",
        "        likelihood = -np.sum(weights * np.log(np.maximum(method_specific_weights, 1e-6)))  # Ensure non-zero probabilities\n",
        "        return smoothness + likelihood\n",
        "\n",
        "    initial_weights = np.array(edge_weights)\n",
        "    result = minimize(energy_function, initial_weights, method='L-BFGS-B', bounds=[(0, 1)] * len(edge_weights))\n",
        "\n",
        "    return result.x if result.success else initial_weights\n",
        "\n",
        "def merge_edges_with_mrf(methods, method_names, method_auprs, weight_threshold=0):\n",
        "    \"\"\"Merge edges from multiple methods using MRF-based modeling.\"\"\"\n",
        "    method_weights = compute_method_weights(method_auprs)\n",
        "    merged_df = pd.concat(methods, ignore_index=True)\n",
        "    merged_df[\"Method\"] = list(itertools.chain(*[[name] * len(df) for name, df in zip(method_names, methods)]))\n",
        "\n",
        "    grouped = merged_df.groupby([\"Gene1\", \"Gene2\"])\n",
        "    edge_summary = []\n",
        "\n",
        "    for (gene1, gene2), group in grouped:\n",
        "        edge_weights = group[\"EdgeWeight\"].values\n",
        "        method_names_list = group[\"Method\"].values\n",
        "        optimized_weights = mrf_optimization(edge_weights, method_names_list, method_weights)\n",
        "        avg_prob = np.mean(optimized_weights)\n",
        "        edge_summary.append([gene1, gene2, avg_prob])\n",
        "\n",
        "    edge_df = pd.DataFrame(edge_summary, columns=[\"Gene1\", \"Gene2\", \"Probability\"])\n",
        "    credible_edges = edge_df[edge_df[\"Probability\"] >= weight_threshold]\n",
        "\n",
        "    return credible_edges\n",
        "\n",
        "def evaluate_edges(predicted_edges, ref_network):\n",
        "    \"\"\"Evaluate merged edges against the reference network using AUPR and AUROC.\"\"\"\n",
        "    if predicted_edges.empty:\n",
        "        return 0, 0  # Return zero scores if there are no predictions\n",
        "\n",
        "    true_edges = set(zip(ref_network[\"Gene1\"], ref_network[\"Gene2\"]))\n",
        "    predicted_edges_list = list(zip(predicted_edges[\"Gene1\"], predicted_edges[\"Gene2\"], predicted_edges[\"Probability\"]))\n",
        "\n",
        "    y_true = [1 if (gene1, gene2) in true_edges else 0 for gene1, gene2, _ in predicted_edges_list]\n",
        "    y_scores = [float(prob) for _, _, prob in predicted_edges_list]\n",
        "\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
        "    aupr = auc(recall, precision)\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "    auroc = auc(fpr, tpr)\n",
        "\n",
        "    threshold = 0.5  # Default threshold for binary classification\n",
        "    best_f1, best_thresh = 0, 0\n",
        "    for thresh in np.linspace(0, 1, 100):\n",
        "        y_pred = [1 if s >= thresh else 0 for s in y_scores]\n",
        "        f1 = f1_score(y_true, y_pred)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_thresh = thresh\n",
        "    y_pred = [1 if prob >= best_thresh else 0 for prob in y_scores]\n",
        "\n",
        "    precision_metric = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall_metric = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1_metric = f1_score(y_true, y_pred, zero_division=0)\n",
        "    mcc_metric = matthews_corrcoef(y_true, y_pred)\n",
        "    log_loss_metric = log_loss(y_true, y_scores, labels=[0, 1]) if len(set(y_true)) > 1 else None\n",
        "    kappa_metric = cohen_kappa_score(y_true, y_pred)\n",
        "    top_k_acc = top_k_accuracy_score(y_true, np.array(y_scores).reshape(-1, 1), k=1)  # Top-1 Accuracy\n",
        "\n",
        "    return {\n",
        "        \"AUPR\": aupr,\n",
        "        \"AUROC\": auroc,\n",
        "        \"Precision\": precision_metric,\n",
        "        \"Recall\": recall_metric,\n",
        "        \"F1-Score\": f1_metric,\n",
        "        \"MCC\": mcc_metric,\n",
        "        \"Log Loss\": log_loss_metric,\n",
        "        \"Kappa\": kappa_metric,\n",
        "        \"Top-K Acc\": top_k_acc\n",
        "    }\n",
        "\n",
        "\n",
        "def load_files(basefolder):\n",
        "    \"\"\"Load all CSV files from the basefolder and normalize edge weights.\"\"\"\n",
        "    all_files = glob.glob(os.path.join(basefolder, \"*.csv\"))\n",
        "    method_data = {}\n",
        "\n",
        "    for file in all_files:\n",
        "        method_name = os.path.basename(file).split('.')[0]\n",
        "        df = pd.read_csv(file)\n",
        "        # print(df)\n",
        "\n",
        "        if \"EdgeWeight\" in df.columns:\n",
        "            min_val = df[\"EdgeWeight\"].min()\n",
        "            max_val = df[\"EdgeWeight\"].max()\n",
        "            # Avoid division by zero\n",
        "            if max_val > min_val:\n",
        "                df[\"EdgeWeight\"] = (df[\"EdgeWeight\"] - min_val) / (max_val - min_val)\n",
        "            else:\n",
        "                df[\"EdgeWeight\"] = 0.5  # Arbitrary constant if all weights are equal\n",
        "\n",
        "        method_data[method_name] = df\n",
        "        # print(df)\n",
        "\n",
        "    return method_data\n",
        "\n",
        "def main(basefolder, ref_network_file, weight_threshold=0):\n",
        "    \"\"\"Main function to process MRF-based evaluation for all method combinations.\"\"\"\n",
        "    method_data = load_files(basefolder)\n",
        "    ref_network = pd.read_csv(os.path.join(basefolder, ref_network_file))\n",
        "    excluded_files = {\"Final_XGBoost_MPCM\", \"refNetwork\", \"coffee_rankedEdges\", \"Final_XGBoost_PCM\"}\n",
        "    method_names = [name for name in method_data.keys() if name not in excluded_files]\n",
        "\n",
        "    results = []\n",
        "    MAX_COMBO_SIZE = 5\n",
        "\n",
        "    for r in range(2, min(len(method_names), MAX_COMBO_SIZE) + 1):\n",
        "        for combo in itertools.combinations(method_names, r):\n",
        "            selected_methods = [method_data[method] for method in combo]\n",
        "            merged_edges = merge_edges_with_mrf(selected_methods, combo, method_auprs, weight_threshold=weight_threshold)\n",
        "\n",
        "            if not merged_edges.empty:\n",
        "                metrics = evaluate_edges(merged_edges, ref_network)\n",
        "                results.append({\"Combination\": combo, **metrics})\n",
        "\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    output_dir = os.path.join(basefolder, \"myresult\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_file = os.path.join(output_dir, \"evaluation_results_MRF_Fuse.csv\")\n",
        "    results_df.to_csv(output_file, index=False)\n",
        "    print(f\"Evaluation results saved to {output_file}\")\n",
        "\n",
        "def process_all_subfolders(main_function, base_folder, csv_file, weight_threshold):\n",
        "    \"\"\"Process all subfolders and apply the MRF-based evaluation.\"\"\"\n",
        "    subfolders = [\n",
        "        os.path.join(base_folder, name)\n",
        "        for name in os.listdir(base_folder)\n",
        "        if os.path.isdir(os.path.join(base_folder, name))\n",
        "        and not (name.endswith(\"-50\") or name.endswith(\"-70\"))\n",
        "        and not contains_evaluation_results_mrf(os.path.join(base_folder, name))  # Avoid redundant processing\n",
        "    ]\n",
        "\n",
        "    for subfolder in subfolders:\n",
        "        print(f\"Processing folder: {subfolder}\")\n",
        "        main_function(subfolder, csv_file, weight_threshold)\n",
        "\n",
        "names = [\"GSD\"]\n",
        "for name in names:\n",
        "    process_all_subfolders(main_function=main, base_folder=\"your folder path\" + name, csv_file=\"reference network path\", weight_threshold=0)\n"
      ]
    }
  ]
}