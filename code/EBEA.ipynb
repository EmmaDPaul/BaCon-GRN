{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Inputs should be the gene network obtained from GRN inference techniques. It should have the columns \"Gene1\",\"Gene2\", \"EdgeWeight\" and \"Direction\". The sample files are provided in the data folder."
      ],
      "metadata": {
        "id": "0vs9yVsYbH1E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q00yZv0vZgvc"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def smart_read_csv(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        # Read a small portion to detect delimiter\n",
        "        sample = f.read(2048)\n",
        "        sniffer = csv.Sniffer()\n",
        "        try:\n",
        "            dialect = sniffer.sniff(sample, delimiters=[',', '\\t'])\n",
        "        except csv.Error:\n",
        "            # Fallback if delimiter can't be detected\n",
        "            dialect = csv.get_dialect('excel')  # defaults to comma\n",
        "        f.seek(0)\n",
        "        return pd.read_csv(f, sep=dialect.delimiter)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import itertools\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve, auc, precision_score, recall_score, f1_score, matthews_corrcoef, log_loss, cohen_kappa_score, top_k_accuracy_score\n",
        "\n",
        "method_alphas = {'GRNBOOST2': 0.55, 'SINCERITIES': 0.42, 'GENIE3': 0.56, 'PPCOR': 0.38}\n",
        "\n",
        "def contains_evaluation_results_bayesian(subfolder):\n",
        "    for root, dirs, files in os.walk(subfolder):\n",
        "        if \"evaluation_results_EBEA.csv\" in files:\n",
        "            return True\n",
        "def compute_method_weights(method_alphas):\n",
        "    min_aupr, max_aupr = min(method_alphas.values()), max(method_alphas.values())\n",
        "    method_weights = {method: (aupr - min_aupr) / (max_aupr - min_aupr) + 0.1 for method, aupr in method_alphas.items()}\n",
        "    return method_weights\n",
        "\n",
        "def hierarchical_bayesian_update(edge_weights, method_names, method_weights):\n",
        "    \"\"\"Perform Hierarchical Bayesian Update based on latent edge trustworthiness.\"\"\"\n",
        "    global_mu = np.mean(edge_weights)\n",
        "    global_sigma = np.std(edge_weights) + 1e-6  # Avoid zero std\n",
        "\n",
        "    weighted_sum = sum(method_weights[method] * edge for edge, method in zip(edge_weights, method_names))\n",
        "    weight_sum = sum(method_weights[method] for method in method_names)\n",
        "\n",
        "    edge_trustworthiness = (weighted_sum / weight_sum) if weight_sum > 0 else global_mu\n",
        "    credibility_score = norm.cdf(edge_trustworthiness, loc=global_mu, scale=global_sigma)\n",
        "\n",
        "    return edge_trustworthiness, credibility_score\n",
        "\n",
        "def merge_edges_with_hierarchical_bayesian(methods, method_names, method_alphas, weight_threshold=0):\n",
        "    method_weights = compute_method_weights(method_alphas)\n",
        "    merged_df = pd.concat(methods, ignore_index=True)\n",
        "    merged_df[\"Method\"] = list(itertools.chain(*[[name] * len(df) for name, df in zip(method_names, methods)]))\n",
        "\n",
        "    grouped = merged_df.groupby(\"Direction\")\n",
        "    edge_summary = []\n",
        "\n",
        "    for name, group in grouped:\n",
        "        edge_weights = group[\"EdgeWeight\"].values\n",
        "        method_names_list = group[\"Method\"].values\n",
        "        trustworthiness, credibility = hierarchical_bayesian_update(edge_weights, method_names_list, method_weights)\n",
        "        edge_summary.append([name, trustworthiness, credibility])\n",
        "\n",
        "    edge_df = pd.DataFrame(edge_summary, columns=[\"Direction\", \"Trustworthiness\", \"Credibility\"])\n",
        "    edge_df = edge_df.merge(merged_df[[\"Direction\", \"Gene1\", \"Gene2\"]].drop_duplicates(), on=\"Direction\", how=\"left\")\n",
        "    credible_edges = edge_df[edge_df[\"Credibility\"] >= weight_threshold]\n",
        "\n",
        "    return credible_edges\n",
        "\n",
        "def evaluate_edges(predicted_edges, ref_network):\n",
        "    true_edges = set(ref_network[\"Direction\"])\n",
        "    predicted_edges_list = list(zip(predicted_edges[\"Direction\"], predicted_edges[\"Credibility\"]))\n",
        "\n",
        "    y_true = [1 if direction in true_edges else 0 for direction, _ in predicted_edges_list]\n",
        "    y_scores = [float(prob) for _, prob in predicted_edges_list]\n",
        "\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
        "    aupr = auc(recall, precision)\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "    auroc = auc(fpr, tpr)\n",
        "\n",
        "    threshold = 0.5  # Default threshold for binary classification\n",
        "    y_pred = [1 if prob >= threshold else 0 for prob in y_scores]\n",
        "\n",
        "    precision_metric = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall_metric = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1_metric = f1_score(y_true, y_pred, zero_division=0)\n",
        "    mcc_metric = matthews_corrcoef(y_true, y_pred)\n",
        "    log_loss_metric = log_loss(y_true, y_scores, labels=[0, 1]) if len(set(y_true)) > 1 else None\n",
        "    kappa_metric = cohen_kappa_score(y_true, y_pred)\n",
        "    top_k_acc = top_k_accuracy_score(y_true, np.array(y_scores).reshape(-1, 1), k=1)  # Top-1 Accuracy\n",
        "\n",
        "    return {\n",
        "        \"AUPR\": aupr,\n",
        "        \"AUROC\": auroc,\n",
        "        \"Precision\": precision_metric,\n",
        "        \"Recall\": recall_metric,\n",
        "        \"F1-Score\": f1_metric,\n",
        "        \"MCC\": mcc_metric,\n",
        "        \"Log Loss\": log_loss_metric,\n",
        "        \"Kappa\": kappa_metric,\n",
        "        \"Top-K Acc\": top_k_acc\n",
        "    }\n",
        "def load_files(basefolder):\n",
        "    \"\"\"Load all CSV files from the basefolder and normalize edge weights.\"\"\"\n",
        "    all_files = glob.glob(os.path.join(basefolder, \"*.csv\"))\n",
        "    method_data = {}\n",
        "\n",
        "    for file in all_files:\n",
        "        method_name = os.path.basename(file).split('.')[0]\n",
        "        df = smart_read_csv(file)\n",
        "        print(\"df columns\")\n",
        "        print(df.columns)\n",
        "        # Check if 'Direction' column (case-sensitive) is missing\n",
        "        if \"Direction\" not in df.columns:\n",
        "            # Determine which case of Gene1 and Gene2 is present\n",
        "            gene1_col = \"Gene1\" if \"Gene1\" in df.columns else (\"gene1\" if \"gene1\" in df.columns else None)\n",
        "            gene2_col = \"Gene2\" if \"Gene2\" in df.columns else (\"gene2\" if \"gene2\" in df.columns else None)\n",
        "\n",
        "            # Ensure both columns are found\n",
        "            if gene1_col and gene2_col:\n",
        "                # Create the 'Direction' column\n",
        "                df[\"Direction\"] = df[gene1_col].astype(str) + \" -> \" + df[gene2_col].astype(str)\n",
        "            else:\n",
        "                raise ValueError(\"Required columns 'Gene1'/'gene1' and/or 'Gene2'/'gene2' are missing.\")\n",
        "\n",
        "        if \"EdgeWeight\" in df.columns:\n",
        "            min_val = df[\"EdgeWeight\"].min()\n",
        "            max_val = df[\"EdgeWeight\"].max()\n",
        "            # Avoid division by zero\n",
        "            if max_val > min_val:\n",
        "                df[\"EdgeWeight\"] = (df[\"EdgeWeight\"] - min_val) / (max_val - min_val)\n",
        "            else:\n",
        "                df[\"EdgeWeight\"] = 0.5  # Arbitrary constant if all weights are equal\n",
        "\n",
        "        method_data[method_name] = df\n",
        "\n",
        "    return method_data\n",
        "def main(basefolder, ref_network_file, weight_threshold=0):\n",
        "    method_data = load_files(basefolder)\n",
        "    ref_network = smart_read_csv(os.path.join(basefolder, ref_network_file))\n",
        "    if \"Direction\" not in ref_network.columns:\n",
        "        # Determine which case of Gene1 and Gene2 is present\n",
        "        gene1_col = \"Gene1\" #if \"Gene1\" in df.columns else (\"gene1\" if \"gene1\" in df.columns else None)\n",
        "        gene2_col = \"Gene2\" #if \"Gene2\" in df.columns else (\"gene2\" if \"gene2\" in df.columns else None)\n",
        "\n",
        "        # Ensure both columns are found\n",
        "        if gene1_col and gene2_col:\n",
        "            # Create the 'Direction' column\n",
        "            ref_network[\"Direction\"] = ref_network[gene1_col].astype(str) + \" -> \" + ref_network[gene2_col].astype(str)\n",
        "        else:\n",
        "            raise ValueError(\"Required columns 'Gene1'/'gene1' and/or 'Gene2'/'gene2' are missing.\")\n",
        "\n",
        "    # Check if \"Direction\" column is missing\n",
        "    if \"Direction\" not in ref_network.columns:\n",
        "        # Create the \"Direction\" column by combining \"Gene1\" and \"Gene2\"\n",
        "        ref_network[\"Direction\"] = ref_network[\"Gene1\"].astype(str) + \" -> \" + ref_network[\"Gene2\"].astype(str)\n",
        "    excluded_files = {\"Final_XGBoost_MPCM\", \"refNetwork\", \"coffee_rankedEdges\", \"Final_XGBoost_PCM\"}\n",
        "    method_names = [name for name in method_data.keys() if name not in excluded_files]\n",
        "\n",
        "    results = []\n",
        "    MAX_COMBO_SIZE = 5\n",
        "\n",
        "    for r in range(2, min(len(method_names), MAX_COMBO_SIZE) + 1):\n",
        "        for combo in itertools.combinations(method_names, r):\n",
        "            selected_methods = [method_data[method] for method in combo]\n",
        "            merged_edges = merge_edges_with_hierarchical_bayesian(selected_methods, combo, method_auprs, weight_threshold=weight_threshold)\n",
        "            metrics = evaluate_edges(merged_edges, ref_network)\n",
        "            results.append({\"Combination\": combo, **metrics})\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    output_dir = os.path.join(basefolder, \"myresult\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    results_df.to_csv(os.path.join(output_dir, \"evaluation_results_EBEA.csv\"), index=False)\n",
        "    print(\"Evaluation results saved.\")\n",
        "\n",
        "def process_all_subfolders(main_function, base_folder, csv_file, weight_threshold):\n",
        "    subfolders = [\n",
        "        os.path.join(base_folder, name)\n",
        "        for name in os.listdir(base_folder)\n",
        "        if os.path.isdir(os.path.join(base_folder, name))\n",
        "        and not contains_evaluation_results_bayesian(os.path.join(base_folder, name))\n",
        "    ]\n",
        "    for subfolder in subfolders:\n",
        "        print(f\"Processing folder: {subfolder}\")\n",
        "        main_function(subfolder, csv_file, weight_threshold)\n",
        "\n",
        "names = [\"GSD\"]\n",
        "for name in names:\n",
        "    process_all_subfolders(main_function=main, base_folder=\"your folder path\" + name, csv_file=\"reference network path\", weight_threshold=0)\n"
      ],
      "metadata": {
        "id": "Tx0F-EJkZ44z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}